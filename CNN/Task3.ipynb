{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (1.26.2)\n",
      "Requirement already satisfied: promise in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\fpt\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (4.66.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\fpt\\appdata\\roaming\\python\\python311\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.11.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\fpt\\appdata\\roaming\\python\\python311\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\fpt\\appdata\\roaming\\python\\python311\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\fpt\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.63.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn code tiếp theo thực hiện việc khởi dựng mô hình CNN với kiến trúc để phân loại cho 11 lớp khuôn mặt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a model to classify images of 11 classes\n",
    "def gen_model():\n",
    "    # Defines & compiles the model\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150,\n",
    "    3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    keras.layers.Dropout(rate=0.15), #adding dropout regularization throughout the model to deal with overfitting\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Dropout(rate=0.10),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # 3 output neuron for the 3 classes of Animal Images\n",
    "    tf.keras.layers.Dense(11, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    from tensorflow.keras.optimizers import RMSprop\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo chúng ta thiết lập dữ liệu và gọi mô hình để huấn luyện và dự báo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a model to classify images of 3 classes: cat, dog, and panda\n",
    "def train_test_animals():\n",
    "    # Creates an instance of an ImageDataGenerator called train_datagen, and a train_generator, train_datagen.flow_from_directory\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    #splits data into training and testing(validation) sets\n",
    "    train_datagen =ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #training data\n",
    "    train_generator = train_datagen.flow_from_directory('/face_data/face',\n",
    "    # Source directory\n",
    "    target_size=(150, 150), # Resizes images\n",
    "    batch_size=15,\n",
    "    class_mode='categorical',subset = 'training')\n",
    "    epochs = 1\n",
    "    #Testing data\n",
    "    validation_generator = train_datagen.flow_from_directory('/face_data/validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=5,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "    model = gen_model()\n",
    "    #Model fitting for a number of epochs\n",
    "    history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=150,\n",
    "    epochs=epochs,\n",
    "    validation_data = validation_generator,\n",
    "\n",
    "    validation_steps = 50,\n",
    "    verbose=1)\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    #This code is used to plot the training and validation accuracy\n",
    "    epochs_range = range(1, epochs + 1)  # Thay đổi đoạn này\n",
    "    \n",
    "    model.save_weights('/saved_weights.h5')\n",
    "    # returns accuracy of training\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "    # Dự đoán cho tập train\n",
    "    train_predictions = model.predict(train_generator)\n",
    "    train_labels = train_generator.classes\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions.argmax(axis=1))\n",
    "    train_precision = precision_score(train_labels, train_predictions.argmax(axis=1), average='weighted')\n",
    "    train_recall = recall_score(train_labels, train_predictions.argmax(axis=1), average='weighted')\n",
    "\n",
    "    # Dự đoán cho tập test\n",
    "    test_predictions = model.predict(validation_generator)\n",
    "    test_labels = validation_generator.classes\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions.argmax(axis=1))\n",
    "    test_precision = precision_score(test_labels, test_predictions.argmax(axis=1), average='weighted')\n",
    "    test_recall = recall_score(test_labels, test_predictions.argmax(axis=1), average='weighted')\n",
    "\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Train Precision:\", train_precision)\n",
    "    print(\"Train Recall:\", train_recall)\n",
    "\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"Test Precision:\", test_precision)\n",
    "    print(\"Test Recall:\", test_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 132 images belonging to 11 classes.\n",
      "Found 33 images belonging to 11 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FPT\\AppData\\Local\\Temp\\ipykernel_4936\\1176710479.py:24: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/150 [>.............................] - ETA: 30s - loss: 4.4963 - acc: 0.0909WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 150 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "150/150 [==============================] - 5s 17ms/step - loss: 4.4963 - acc: 0.0909 - val_loss: 2.4075 - val_acc: 0.1515\n",
      "9/9 [==============================] - 2s 33ms/step\n",
      "5/7 [====================>.........] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FPT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 15ms/step\n",
      "Train Accuracy: 0.09090909090909091\n",
      "Train Precision: 0.008391608391608392\n",
      "Train Recall: 0.09090909090909091\n",
      "Test Accuracy: 0.09090909090909091\n",
      "Test Precision: 0.00879765395894428\n",
      "Test Recall: 0.09090909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FPT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "train_test_animals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do dữ liệu đầu vào ít nên được sử dụng hết trong quá trình huấn luyện mô hình. Khiến cho  quá trình huấn luyện đã bị gián đoạn và đưa ra kết quả với độ chính xác thâp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết hợp giảm chiều và sử dụng mô hình ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15151515151515152\n",
      "Precision: 0.19090909090909092\n",
      "Recall: 0.16666666666666666\n",
      "F1 Score: 0.15151515151515152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FPT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train_dir = '/face_data/face'\n",
    "validation_dir = '/face_data/validation'\n",
    "\n",
    "# Function to resize images\n",
    "def resize_image(image_path, target_size=(150, 150)):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize(target_size).convert(\"RGB\")\n",
    "    return np.array(image)\n",
    "\n",
    "def load_data(train_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(train_dir):\n",
    "        \n",
    "        label_dir = os.path.join(train_dir, label)\n",
    "        for file in os.listdir(label_dir):\n",
    "            image_path = os.path.join(label_dir, file)\n",
    "            # Resize image to 150x150\n",
    "            try:\n",
    "                resized_image = resize_image(image_path)\n",
    "                images.append(resized_image)\n",
    "                # Assign label based on folder name\n",
    "                if label == 'centerlight':\n",
    "                    labels.append(0)  \n",
    "                elif label == 'glasses':\n",
    "                    labels.append(1)\n",
    "                elif label == 'happy':\n",
    "                    labels.append(2)\n",
    "                elif label == 'leftlight':\n",
    "                    labels.append(3)  \n",
    "                elif label == 'noglasses':\n",
    "                    labels.append(4)\n",
    "                elif label == 'normal':\n",
    "                    labels.append(5)\n",
    "                elif label == 'rightlight':\n",
    "                    labels.append(6)\n",
    "                elif label == 'sad':\n",
    "                    labels.append(7)  \n",
    "                elif label == 'sleepy':\n",
    "                    labels.append(8)   \n",
    "                elif label == 'surprise':\n",
    "                    labels.append(9)\n",
    "                elif label == 'wink':\n",
    "                    labels.append(10)                                                           \n",
    "            except Exception as e:\n",
    "                print(\"Error processing image:\", image_path)\n",
    "                print(\"Error message:\", str(e))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "images, labels = load_data(train_dir)\n",
    "\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Flatten the images\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train a Softmax Regression model\n",
    "model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "model.fit(X_train_flatten, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_flatten)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
